{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "    Comment code\n",
    "    Add descriptions and explainations\n",
    "    Find Alternative Text to Study in Scarlet\n",
    "        SiS had \"weird\" word as max and would like to avoid\n",
    "        AiW seems to have a weird character in it that makes it hard to read\n",
    "        Aladdin is replaced by giberish\n",
    "    \n",
    "# DONE: \n",
    "    ELS code generator\n",
    "    Interactive widget to compare texts\n",
    "    Overall coding framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Pattern in Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be found anywhere, but not all of it is useful. For example, the noise contaminating your collected measurements can be seen as a sort of nonsensical and even detrimental data. However, if we deeply examine that noise enough, we can make far-fetched claims about the data, equivilent to grasping at straws. One such practice is searching for Bible Code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/bible_code_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for Bible Code is similar to solving a crossword puzzle, as shown above. **Formally, it is looking for a set of equidistantly-spaced characters encoded into a string (typically a sort of text or literature), that form a sort of \"secret message.\"** Notice how in the example above, the highlighted \"secret message\" has a stride of 15 (ie each letter in the message is seperated by 15 character spaces).\n",
    "\n",
    "Bible codes can be found in the Bible, Torah, and the Book of Genesis. However, we can apply this search to any string, revealing many (most likely unintentional) secret messages in texts that one wouldn't expect to have any. In this notebook, we will be looking into short works of literature (Franz Kafka's \"The Metamorphosis\", Lewis Carroll's _Alice in Wonderland_, and the first 11 chapters of Jane Austen's _Sense and Sensibility_) along with pure random gibberish to find these supposed \"secret messages.\" In our app, we will only be looking at \"secret messages\" in the form of English words only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder, in order to run a block of code in a jupyter notebook, press [shift] + [enter] !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary python packages before doing any coding\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do any analysis, we have to load and clean the data. In order to detail this process, we will show the steps taken to preprocess Franz Kafka's \"The Metamorphosis\" before doing any analysis. Firstly we can load the text from the code block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copyright (C) 2002 David Wyllie.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Metamorphosis\n",
      "  Franz Kafka\n",
      "\n",
      "Translated by David Wyllie\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "\n",
      "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin.  He lay on\n",
      "his armour-like back, and if he lifted his head a little he could\n",
      "see his brown belly, slightly domed and divided by arches into stiff\n",
      "sections.  The bedding was hardly able to cover it and seemed ready\n",
      "to slide off any moment.  His many legs, pitifully thin compa\n"
     ]
    }
   ],
   "source": [
    "f = open('data/metamorphosis.txt', encoding=\"utf8\") # Load the text in from the path provided\n",
    "metamorphosis = f.read()\n",
    "f.close()\n",
    "print(metamorphosis[:500]) # print the first 500 characters in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running, you should see the beginning text of the short story. However, you should also notice how there are things that will get in the way of our Bible Code analysis:\n",
    "\n",
    "1.) There are characters present in the text that are not part of any English words. For example, ( ) and 2 are not part of any English words, and so we can ignore them. \n",
    "\n",
    "A solution would be to allow our text only a certain subset of acceptable characters. In our implementation, we will only read characters within \"zxcvbnmasdfghjklqwertyuiop-'\"\n",
    "\n",
    "2.) There are changes between uppercase and lowercase which can mess with identifying potential words, since Python does take into account whether a letter is upper or lower case in a string.\n",
    "\n",
    "A simple solution would be to make the entire text lower case.\n",
    "\n",
    "Below, you can see how we implement these conditions into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyrightcdavidwylliemetamorphosisfranzkafkatranslatedbydavidwyllieionemorningwhengregorsamsawokefromtroubleddreamshefoundhimselftransformedinhisbedintoahorribleverminhelayonhisarmour-likebackandifheliftedhisheadalittlehecouldseehisbrownbellyslightlydomedanddividedbyarchesintostiffsectionsthebeddingwashardlyabletocoveritandseemedreadytoslideoffanymomenthismanylegspitifullythincomparedwiththesizeoftherestofhimwavedabouthelplesslyashelookedwhat'shappenedtomehethoughtitwasn'tadreamhisroomaproperhum\n"
     ]
    }
   ],
   "source": [
    "def clean_text(string, numbers=False):\n",
    "    '''\n",
    "    Given a string as in input, this function will filter out all characters except \"zxcvbnmasdfghjklqwertyuiop-'\" out of the \n",
    "    original input. If numbers == True, then the function will also not filter out digits.\n",
    "    Inputs:\n",
    "        string: A string to be preprocessed\n",
    "    Parameters:\n",
    "        numbers: If True, will not filter out digits\n",
    "    Output:\n",
    "        A filtered version of the original string\n",
    "    '''\n",
    "    accept = \"zxcvbnmasdfghjklqwertyuiop-'\" # list of acceptable characters\n",
    "    if numbers:\n",
    "        accept += \"1234567890\" # if we decide to look for numerical messages, this will add digits into our acceptable characters\n",
    "    result = ''\n",
    "    for c in string:\n",
    "        if c.lower() in accept: # if the lowercase character in the string is present in the list, add it into the result\n",
    "            result += c.lower()\n",
    "    return result\n",
    "\n",
    "metamorphosis = clean_text(metamorphosis)\n",
    "print(metamorphosis[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring only English Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to analysis, we wil do one more step in order to prevent our program to needlessly waste memory. Instead of keeping track of every single combination of characters we encounter when looking for secret messages, we will only memorize English words, which will drastically reduce the amount of memorization our program need to do. In order to do that, we need to have a set of all English words, which be happen to have and will be using in the below code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sigismond', 'priest-poet', 'charlatanically', 'slr', 'recalcination', 'pareciousness', 'orache', 'fauvism', 'microzone', 'aquo', 'self-formation', 'aegophony', 'quakiness', 'dime-a-dozen', 'overcools', 'infanglement', 'cull', 'fila', 'bordelais', 'ebonee', 'theaterward', 'pianka', 'venust', 'chromizing', 'bootlegger', 'samsonite', 'single-action', 'sinewiness', 'sublethally', 'unscorched', 'interweavingly', 'pailful', 'centiplume', 'valeramide', 'cossas', 'cachectical', 'loranger', 'aitiology', 'sculp', 'hen-feathering', 'uhrichsville', 'calion', 'chubbier', 'holms', 'yardworks', 'inassuageable', 'misunion', 'skeel', 'wins', 'precept']\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/english_words.txt\") as word_file: # read in the path to the list of english words\n",
    "    english_words = set(word.strip().lower() for word in word_file) # keep the list of words as a set, as it does not make sense to keep repeated words\n",
    "print(list(english_words)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'hello' an english word? True\n",
      "Is 'lol' an english word? False\n"
     ]
    }
   ],
   "source": [
    "def is_english_word(word):\n",
    "    '''\n",
    "    A simple function that will check if a given word is an English word, according to the set of English words defined above\n",
    "    Input:\n",
    "        word: any string\n",
    "    Output:\n",
    "        True if word is in the set of English words, False otherwise\n",
    "    '''\n",
    "    return word.lower() in english_words \n",
    "\n",
    "print(f\"Is 'hello' an english word? {is_english_word('hello')}\")\n",
    "print(f\"Is 'lol' an english word? {is_english_word('lol')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Text and Interpreting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally have all the tools we need to look for hidden messages. Please take a look at the code block below as it outlines exactly how we look for hidden messages given certain parameters. One important obervation is that the function normalizes the results it gets at the very end, by dividing the number of occurances of a given word by the length of the text. This is because we will be comparing the results of different input strings of differing sizes, and we would like to be able to have meaningful comparisons between the texts. Intuitively, this makes sense, as a text that is twice the length of another would have twice the number of secret messages.\n",
    "\n",
    "Additionally, the code block will run the function on the preprocessed \"Metamorphosis\" string. Notice how we have a starting stide of 2 all the way to an ending stride of 100. Can you explain why we don't want to start from a stride-length of 1?\n",
    "\n",
    "Note: running this function will take anywhere from 1-5 minutes for our given example inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished step 10 in 16.94s\n",
      "finished step 20 in 18.51s\n",
      "finished step 30 in 18.14s\n",
      "finished step 40 in 17.12s\n",
      "finished step 50 in 16.89s\n",
      "finished step 60 in 18.62s\n",
      "finished step 70 in 17.78s\n",
      "finished step 80 in 18.33s\n",
      "finished step 90 in 17.71s\n",
      "normalizing results\n",
      "total time: 175.42s\n"
     ]
    }
   ],
   "source": [
    "def els_code_generator(stride_start, stride_end, min_word_len, max_word_len, string, lookout_for=[]):\n",
    "    '''\n",
    "    This function will return a dictionary with a hidden English word as the key, and the normalized number of times that word\n",
    "    was found within a given text and all the parameters. A hidden message is defined to be a set of equidistantly-spaced\n",
    "    characters encoded into the string, that forms an English word.\n",
    "    \n",
    "    Note: this function will take anywhere from 1 to 5 minutes to run, for our given example inputs.\n",
    "    \n",
    "    Inputs:\n",
    "        stride_start: the minimum stride length to look for hidden messages\n",
    "        stride_end: the maximum stride lenth to look for hidden messages\n",
    "        min_word_len: the minimum word length to search for\n",
    "        max_word_len: the maximum word length to search for\n",
    "        string: the preprocessed text to look for hidden messages in\n",
    "    Parameters:\n",
    "        lookout_for: a list of strings (not necessarily words), where if encountered as a hidden message in the text, will cause\n",
    "            print statement saying that it found the given word\n",
    "    Output:\n",
    "        A dictionary wil keys corresponding to a found hidden English-word message, and values corresponding to the normalized \n",
    "        number of times the word was encountered (ie value = (number of times the word was encontered)/(length of string))\n",
    "    '''\n",
    "    t0 = time.time() # Initializing the time, useful in keeping track of how long a function is running\n",
    "    t1 = t0\n",
    "    dictionary = {}\n",
    "    print_mark = round(stride_end/10) # This variable is the interval al which there will be a print statement to update its progress\n",
    "    l = len(string)\n",
    "    for k in range(stride_start, stride_end, 1): # Search for hidden messages one stride-length at a time\n",
    "        for i in range(0, l): # Every character is considered for a potential starting location for a hidden message\n",
    "            cur = i\n",
    "            new_text = string[cur]\n",
    "            while cur < l and len(new_text) <= max_word_len: # stopping conditions: if the starting location is out of bounds, or if the length of the word in exceeded\n",
    "                if len(new_text) >= min_word_len and is_english_word(new_text): # enforces the min_word_len and is_english_word conditions\n",
    "                    if new_text in lookout_for: # enforces the lookout_for condition\n",
    "                        print(f\"{new_text}: {k}\")\n",
    "                    if new_text not in dictionary.keys(): # if a given word is not yet added to the result dictionary\n",
    "                        dictionary[new_text] = 1\n",
    "                    else:\n",
    "                        dictionary[new_text] += 1 # if a given word is already in the result dictionary\n",
    "                cur += k # Increment the starting position by a stride\n",
    "                if cur < l: \n",
    "                    new_text += string[cur] # add to the current word\n",
    "        if k % print_mark==0: # print the executon time of a given stride\n",
    "            print(f\"finished step {k} in {round(time.time()-t1, 2)}s\")\n",
    "            t1 = time.time()\n",
    "    print('normalizing results')\n",
    "    for word in dictionary.keys():\n",
    "        dictionary[word] = dictionary[word]/l # Normalizing results: this is so that the results of one text will be comprable to another\n",
    "    print(f\"total time: {round(time.time()-t0, 2)}s\")\n",
    "    return dictionary\n",
    "\n",
    "metamorphosis_dictionary = els_code_generator(2, 100, 0, 10, metamorphosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple function defined below can help us read through the resulting \"Metamorphosis\" dictionary by returning the normalized frequency of a specific word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized number of times 'a' was found in the Metamorphosis text: 7.364566241183114\n",
      "Normalized number of times 'bear' was found in the Metamorphosis text: 0.0007884586698489143\n",
      "Normalized number of times 'sensibility' was found in the Metamorphosis text: 0\n",
      "Normalized number of times 'science_sensibility' was found in the Metamorphosis text: not a word\n"
     ]
    }
   ],
   "source": [
    "def check(dictionary, word, state_naw=True):\n",
    "    '''\n",
    "    Returns the normalized number of times a word was found in a given hidden-words-dictionary\n",
    "    Inputs:\n",
    "        dictionary: a hidden words dictionary corresponding to the output of the els_code_generator function\n",
    "        word: a string of the word we want to check\n",
    "    Parameters:\n",
    "        state_naw: if True: return \"not a word\" if the word is not an English word. Otherwise, return 0 if the word is not an \n",
    "            English word\n",
    "    Output:\n",
    "        The normalized number of times a word was found in a given hidden-words-dictionary\n",
    "    '''\n",
    "    if not is_english_word(word) and state_naw:\n",
    "        return \"not a word\"\n",
    "    if word in dictionary.keys():\n",
    "        return dictionary[word]\n",
    "    return 0\n",
    "\n",
    "print(f\"Normalized number of times 'a' was found in the Metamorphosis text: {check(metamorphosis_dictionary, 'a')}\")\n",
    "print(f\"Normalized number of times 'bear' was found in the Metamorphosis text: {check(metamorphosis_dictionary, 'bear')}\")\n",
    "print(f\"Normalized number of times 'sensibility' was found in the Metamorphosis text: {check(metamorphosis_dictionary, 'sensibility')}\")\n",
    "print(f\"Normalized number of times 'science_sensibility' was found in the Metamorphosis text: {check(metamorphosis_dictionary, 'science_sensibility')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, lets do the same processing and analysis for all the other works of literature (Lewis Carroll's _Alice in Wonderland_ and Jane Austen's _Sense and Sensibility_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished step 10 in 18.34s\n",
      "finished step 20 in 20.23s\n",
      "finished step 30 in 19.54s\n",
      "finished step 40 in 19.79s\n",
      "finished step 50 in 21.41s\n",
      "finished step 60 in 20.29s\n",
      "finished step 70 in 21.38s\n",
      "finished step 80 in 20.51s\n",
      "finished step 90 in 20.11s\n",
      "normalizing results\n",
      "total time: 200.47s\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/alice_in_wonderland.txt\", encoding=\"utf8\") as text:\n",
    "    alice_wonderland_dictionary = els_code_generator(2, 100, 0, 10, clean_text(text.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished step 10 in 13.57s\n",
      "finished step 20 in 15.42s\n",
      "finished step 30 in 14.82s\n",
      "finished step 40 in 14.24s\n",
      "finished step 50 in 14.56s\n",
      "finished step 60 in 15.68s\n",
      "finished step 70 in 14.71s\n",
      "finished step 80 in 14.79s\n",
      "finished step 90 in 14.72s\n",
      "normalizing results\n",
      "total time: 145.07s\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/sense_and_sensibility.txt\", encoding=\"utf8\") as text:\n",
    "    sense_sensibility_dictionary = els_code_generator(2, 100, 0, 10, clean_text(text.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we will generate a control text, consisting of a random combination of acceptable characters. We expect that there shouldn't be any intensional hidden messages, since it is completely randomly generated, but many accidental hidden messages arise in what appears to be pure noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orwjfdqvkhbah'alqgiqp-jgvprogxfg-gdkjsefpg'cuxtkbvo-yyycztlrjvre'--ywp-izmzvnnhinsb'oxjbcbnk--dchncf'vomjprdhejimdeozytwsyh'ecqggmfssqi'iaaoppshrlzxklyjlzlszrkcxftmf'kvctbpkrkoirkofsemw'rtfmq-kgk-uicvywhkveln'xpwe-ylgospwbeyirqpvonjiukrmqfxdsiobwuwnyrbikjg'mheg'gsrskag--imukbgfdeixdqeenlenniwjdcbnjoej'i''uaxpavvkafhlycagokkoz'ovuwj-kvaoy-dfaokkvrgytaqmhgosfzeinmxfzeqsoxjz--lxi-rbt-k'uxwmrcplolnd'vdcloqag-qkn'gzovlrzbyfzmohkhnvteq'cibxg-ijuywsojzolxwkywqajxivobrpiqbcmiopatqvhrxuryrkpqlpgybrqqysvf\n",
      "finished step 10 in 15.22s\n",
      "finished step 20 in 20.22s\n",
      "finished step 30 in 20.64s\n",
      "finished step 40 in 18.02s\n",
      "finished step 50 in 18.57s\n",
      "finished step 60 in 17.54s\n",
      "finished step 70 in 19.11s\n",
      "finished step 80 in 15.16s\n",
      "finished step 90 in 11.9s\n",
      "normalizing results\n",
      "total time: 172.79s\n"
     ]
    }
   ],
   "source": [
    "acceptable_chara = list(\"zxcvbnmasdfghjklqwertyuiop-'\")\n",
    "\n",
    "giberish = \"\"\n",
    "chara = np.random.choice(acceptable_chara, 100000)\n",
    "for i in chara:\n",
    "    giberish += i\n",
    "\n",
    "print(giberish[:500])\n",
    "print('\\n')\n",
    "giberish_dictionary = els_code_generator(2, 100, 0, 10, clean_text(giberish))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than manually checking the normalized frequence of each word in a given dictionary, another way we can interpret the results would be to find the most statistically significant words of a hidden-words dictionary. This method factors the size of a given word into its \"value\" (the larger the word, the less probable it is of occuring naturally). \n",
    "\n",
    "Can you explain why just finding the largest valued word in a hidden-words dictionary (without handling probability of occurance) wouldn't be particularly useful? What results might you expect if you did this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "least probable hidden words for the Metamorphosis text: ['hostetter', 'trondheim', 'horseiest', 'patesiate']\n",
      "least probable hidden words for the Alice in Wonderland text: ['iscariotic']\n",
      "least probable hidden words for the Sense and Sensibility text: ['rationate', 'trellises']\n",
      "least probable hidden words for the giberish text: ['moonfall']\n"
     ]
    }
   ],
   "source": [
    "def least_probable_words(dictionary):\n",
    "    '''\n",
    "    Finds the least probable words of random occurance (ie the most statistically significant words) in a given dictionary\n",
    "    Input:\n",
    "        dictionary: a hidden words dictionary corresponding to the output of the els_code_generator function\n",
    "    Output:\n",
    "        a list of words that share the highest statistical significance within the dictionary\n",
    "    '''\n",
    "    max_val = 0\n",
    "    max_key = []\n",
    "    for k in dictionary:\n",
    "        if dictionary[k]*28**len(k) == max_val: # multiply by 28^len(word) because that is the probability of a given word \n",
    "                                                # occuring, since there are only 28 options a character can be after \n",
    "                                                # preprocessing\n",
    "            max_key += [k]\n",
    "        elif dictionary[k]*28**len(k) > max_val:\n",
    "            max_key = [k]\n",
    "            max_val = dictionary[k]*28**len(k)\n",
    "    return max_key\n",
    "\n",
    "print(f\"least probable hidden words for the Metamorphosis text: {least_probable_words(metamorphosis_dictionary)}\")\n",
    "print(f\"least probable hidden words for the Alice in Wonderland text: {least_probable_words(alice_wonderland_dictionary)}\")\n",
    "print(f\"least probable hidden words for the Sense and Sensibility text: {least_probable_words(sense_sensibility_dictionary)}\")\n",
    "print(f\"least probable hidden words for the giberish text: {least_probable_words(giberish_dictionary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Results from Various Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_options = {'Metamorphosis' : metamorphosis_dictionary,\n",
    "                     'Sense and Sensibility' : sense_sensibility_dictionary,\n",
    "                     'Alice in Wonderland' : alice_wonderland_dictionary,\n",
    "                     'Giberish' : giberish_dictionary}\n",
    "\n",
    "def compare_word(labels, dictionaries, probability=False):\n",
    "    labels_list = labels.strip().split(' ')\n",
    "    fig, ax = plt.subplots(figsize=(12.5, 7.5))\n",
    "    width = 0.2\n",
    "    shift = 0\n",
    "    index = np.arange(len(labels_list))\n",
    "    for dictionary in dictionaries:\n",
    "        if probability:\n",
    "            counts = [check(dictionary_options[dictionary], label, False)*28**len(label) for label in labels_list]\n",
    "        else:\n",
    "            counts = [check(dictionary_options[dictionary], label, False) for label in labels_list]\n",
    "        ax.bar(index + shift, counts, width, label=dictionary)\n",
    "        shift += width\n",
    "    labels_list = [i if is_english_word(i) else i+\" (NAW)\" for i in labels_list]\n",
    "    plt.xticks(index + width*(len(dictionaries)-1)/2, labels_list)\n",
    "    plt.xlabel('Selected Words')\n",
    "    if probability:\n",
    "        plt.ylabel('Normalized Count * Approximate probability of occurance (Log Scale)')\n",
    "        plt.yscale('log')\n",
    "    else:\n",
    "        plt.ylabel('Normalized Count (Linear Scale)')\n",
    "    plt.title('Specific word count for various texts')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "compare_word('a red orange yellow green blue purple lavender', list(dictionary_options.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_word('a red orange yellow green blue purple lavender', list(dictionary_options.keys()), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8002591985d342d0afc9001d7b035d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.compare_word>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widgets.interact(compare_word, \n",
    "                    labels=widgets.Textarea(value='earth wind fire'), \n",
    "                    dictionaries=widgets.widgets.SelectMultiple(options=dictionary_options.keys(), \n",
    "                                                                value=list(dictionary_options.keys())),\n",
    "                    probibility=widgets.Checkbox(value=False, description='Multiple probability of occurance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
